import joblib
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import os

_processed_df = None
# Ù†Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ø§ Ø§Ù„Ù…ØªØºÙŠØ± Ù„ØªØ®Ø²ÙŠÙ† Ù†Ø³Ø®Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ ØªÙ†Ø¸ÙŠÙÙ‡Ø§ ÙˆÙ…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§ØŒ Ø­ØªÙ‰ Ù†Ø³ØªØ®Ø¯Ù…Ù‡Ø§ ÙÙŠ Ø¯ÙˆØ§Ù„ Ø£Ø®Ø±Ù‰ Ù…Ø«Ù„ Ø§Ù„Ø­ÙØ¸ ÙˆØ§Ù„Ø±Ø³Ù….

# Ø§Ù„Ø© Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø³Ø§ÙØ© Ø¨ÙŠÙ† Ù†Ù‚Ø·ØªÙŠÙ† Ø¹Ù„Ù‰ Ø³Ø·Ø­ Ø§Ù„Ø£Ø±Ø¶ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØµÙŠØºØ© Haversine (ØªØ£Ø®Ø° Ø®Ø·ÙˆØ· Ø§Ù„Ø·ÙˆÙ„ ÙˆØ§Ù„Ø¹Ø±Ø¶).
def haversine_distance(lat1, lon1, lat2, lon2):
    """Calculate Haversine distance between two points (in km)"""
    R = 6371  # Radius of Earth in km | Ù†ØµÙ Ù‚Ø·Ø± Ø§Ù„Ø£Ø±Ø¶ Ø¨Ø§Ù„ÙƒÙŠÙ„ÙˆÙ…ØªØ±Ø§Øª
    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2]) ## ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¯Ø±Ø¬Ø§Øª Ø¥Ù„Ù‰ Ø±Ø§Ø¯ÙŠØ§Ù† 
     
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    
    # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† Ø®Ø· Ø§Ù„Ø¹Ø±Ø¶ ÙˆØ®Ø· Ø§Ù„Ø·ÙˆÙ„ Ø¨ÙŠÙ† Ø§Ù„Ù†Ù‚Ø·ØªÙŠÙ†
    a = np.sin(dlat / 2) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) ** 2 # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø³Ø§ÙØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØµÙŠØºØ© Ù‡Ø§ÙÙŠØ±Ø³ÙŠÙ†
    #  ØµÙŠØºØ© Ù‡Ø§ÙØ±Ø³ÙŠÙ† Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…Ø³Ø§ÙØ© Ø§Ù„Ø²Ø§ÙˆÙŠØ©.
    #  ØµÙŠØºØ© Ù‡Ø§ÙØ±Ø³ÙŠÙ† Ù‡ÙŠ ØµÙŠØºØ© Ø±ÙŠØ§Ø¶ÙŠØ© ØªØ³ØªØ®Ø¯Ù… Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø³Ø§ÙØ© Ø¨ÙŠÙ† Ù†Ù‚Ø·ØªÙŠÙ† Ø¹Ù„Ù‰ Ø³Ø·Ø­ Ø§Ù„ÙƒØ±Ø© Ø§Ù„Ø£Ø±Ø¶ÙŠØ©.
    #  ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø²Ø§ÙˆÙŠØ© Ø¨ÙŠÙ† Ø§Ù„Ù†Ù‚Ø·ØªÙŠÙ† Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„Ù…Ø³Ø§ÙØ© Ø§Ù„Ø®Ø·ÙŠØ©.
    #  ØªØ³ØªØ®Ø¯Ù… Ù‡Ø°Ù‡ Ø§Ù„ØµÙŠØºØ© ÙÙŠ Ø¹Ù„Ù… Ø§Ù„Ø¬ØºØ±Ø§ÙÙŠØ§ ÙˆØ§Ù„Ù…Ù„Ø§Ø­Ø© Ø§Ù„Ø¬ÙˆÙŠØ© ÙˆØ§Ù„Ø¨Ø­Ø±ÙŠØ©.
    #  ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø­Ø³Ø§Ø¨ Ø§Ù„Ø²Ø§ÙˆÙŠØ© Ø¨ÙŠÙ† Ø§Ù„Ù†Ù‚Ø·ØªÙŠÙ† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø«Ù„Ø«ÙŠØ©.
    
    
    return 2 * R * np.arcsin(np.sqrt(a))  # ØªØ¹Ø·ÙŠÙƒ Ø§Ù„Ù…Ø³Ø§ÙØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¨Ø§Ù„ÙƒÙŠÙ„ÙˆÙ…ØªØ±Ø§Øª.




# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
def load_data(filepath, use_smote=True):
    global _processed_df

    df = pd.read_csv(filepath) # ÙŠÙ‚Ø±Ø£ Ù…Ù„Ù CSV ÙˆÙŠØ¶Ø¹Ù‡ ÙÙŠ DataFrame.

    # Rename & clean basic columns
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙˆÙ‚Øª Ø¥Ù„Ù‰ Ù†ÙˆØ¹ datetimeØŒ ÙˆØ­Ø°Ù Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ù„Ø£ÙŠ Ù…Ù† Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ù‡Ù…Ø©.
    df.rename(columns={'time': 'datetime'}, inplace=True)
    # Ù†Ø­Ø§ÙˆÙ„ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚ÙŠÙ… ÙÙŠ Ø§Ù„Ø¹Ù…ÙˆØ¯ datetime Ø¥Ù„Ù‰ Ù†ÙˆØ¹ ØªØ§Ø±ÙŠØ®/Ø²Ù…Ù†.
    # Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ØªÙŠ Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ­ÙˆÙŠÙ„Ù‡Ø§ ØªÙØ¬Ø¹Ù„ NaT (Ù…Ø«Ù„ null).
    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')
    df = df.dropna(subset=['datetime', 'latitude', 'longitude', 'magnitude','depth_km','place'])# Ø­Ø°Ù Ø§Ù„ØµÙÙˆÙ Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù‚ÙŠÙ… Ù…ÙÙ‚ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©

    # Add synthetic depth
    # ØªÙˆÙ„ÙŠØ¯ Ø¹Ù…Ù‚ Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ù„ÙƒÙ„ Ø²Ù„Ø²Ø§Ù„ (Ù…Ù† 1 Ø¥Ù„Ù‰ 25 ÙƒÙ…) â€“ Ø§ÙØªØ±Ø§Ø¶!
    df['depth_km'] = np.random.uniform(1, 10, size=len(df))

    # Time features
    df['month'] = df['datetime'].dt.month #Depremin meydana geldiÄŸi ay
    df['dayofweek'] = df['datetime'].dt.dayofweek #Depremin meydana geldiÄŸi gÃ¼n ((0: Pazartesi))
    df['year'] = df['datetime'].dt.year
    df['hour'] = df['datetime'].dt.hour
    df['is_night'] = ((df['hour'] < 6) | (df['hour'] > 20)).astype(int) # 1 Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø²Ù„Ø²Ø§Ù„ ÙÙŠ Ø§Ù„Ù„ÙŠÙ„ØŒ 0 Ø¥Ø°Ø§ ÙƒØ§Ù† ÙÙŠ Ø§Ù„Ù†Ù‡Ø§Ø±.

    # Physical features
    # Ø·Ø§Ù‚Ø© Ø§Ù„Ø²Ù„Ø²Ø§Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØµÙŠØºØ© ØªÙ‚Ø±ÙŠØ¨ÙŠØ© (Ø¨Ù…Ù‚ÙŠØ§Ø³ Ù„ÙˆØºØ§Ø±ÙŠØªÙ…ÙŠ).
    df['energy'] = 10 ** (1.5 * df['magnitude'] + 4.8)#Ù…Ø¹Ø§Ø¯Ù„Ø© ØªÙ‚Ø±ÙŠØ¨ÙŠØ© Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„Ù…Ù†Ø¨Ø¹Ø«Ø© Ù…Ù† Ø§Ù„Ø²Ù„Ø²Ø§Ù„.
    df['severity_ratio'] = df['magnitude'] / (df['depth_km'] + 1) #Ø´Ø¯Ø© Ø§Ù„Ø²Ù„Ø²Ø§Ù„ Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù„Ø¹Ù…Ù‚.
    df['dist_fault'] = haversine_distance(df['latitude'], df['longitude'], 39.0, 34.0) # Ø§Ù„Ù…Ø³Ø§ÙØ© Ù…Ù† Ù†Ù‚Ø·Ø© Ø§Ù„Ø²Ù„Ø²Ø§Ù„ Ø¥Ù„Ù‰ Ø®Ø· Ø§Ù„ØµØ¯Ø¹ (Ø§ÙØªØ±Ø§Ø¶ÙŠØ§Ù‹ Ø¹Ù†Ø¯ 39.0, 34.0).

    # Severity categories
    # ØªÙ‚Ø³Ù… Ø§Ù„Ø²Ù„Ø§Ø²Ù„ Ø­Ø³Ø¨ magnitude Ø¥Ù„Ù‰ 5 ÙØ¦Ø§Øª Ø´Ø¯Ø©.
    df['severity'] = pd.cut(
        df['magnitude'],
        bins=[0, 3, 4, 5, 6, 10],
        labels=['Hafif', 'Orta', 'GÃ¼Ã§lÃ¼', 'Åiddetli', 'Felaket'],
        right=False
    )
    # Ø­Ø°Ù Ø§Ù„Ø²Ù„Ø§Ø²Ù„ Ø§Ù„Ù„ÙŠ Ø®Ø§Ø±Ø¬ Ø§Ù„ØªÙ‚Ø³ÙŠÙ…. ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ³Ù…ÙŠØ© Ø¥Ù„Ù‰ string.
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙØ¦Ø© Ø¥Ù„Ù‰ Ù†Øµ (Ù…Ø«Ù„: "Åiddetli").
    df = df.dropna(subset=['severity'])
    df['severity_label'] = df['severity'].astype(str)
    
    
    label_mapping = {'Hafif': 0, 'Orta': 1, 'GÃ¼Ã§lÃ¼': 2, 'Åiddetli': 3, 'Felaket': 4}
    df['severity_class'] = df['severity_label'].map(label_mapping)
    print(df[['severity', 'severity_label', 'severity_class']].head())
    
    
#       Feature selection
#       X: Ù…ÙŠØ²Ø§Øª Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.
#       y_class: Ø§Ù„ØªØµÙ†ÙŠÙ (Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙÙŠ).
#       y_reg: Ø§Ù„ØªÙ†Ø¨Ø¤ (Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙ†Ø¨Ø¤ÙŠ).
    features = ['depth_km', 'magnitude', 'latitude', 'longitude', 'month', 'dayofweek',
                'year', 'is_night', 'energy', 'severity_ratio', 'dist_fault']
    X = df[features]
    y_class = df['severity_class']
    y_reg = df[['magnitude', 'depth_km']]



    # Standardization
    # ØªÙˆØ­ÙŠØ¯ ÙƒÙ„ Ø¹Ù…ÙˆØ¯ Ù„ÙŠÙƒÙˆÙ† Ù…ØªÙˆØ³Ø·Ù‡ = 0 ÙˆØ§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ = 1.
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

# ********************************
    # Ø­ÙØ¸ Ø§Ù„Ù€ scaler
    os.makedirs("earthquake_risk_predictor/data", exist_ok=True)
    joblib.dump(scaler, "earthquake_risk_predictor/data/scaler.pkl")
    print("âœ… Scaler saved to 'earthquake_risk_predictor/data/scaler.pkl'")
    
# *********************************
    
    # Apply SMOTE only if required
    # if use_smote:
    #     class_counts = y_class.value_counts()
    #     if all(class_counts >= 2):
    #         k_neighbors = max(1, min(class_counts.min() - 1, 5))
    #         smote = SMOTE(random_state=42, sampling_strategy='not majority', k_neighbors=k_neighbors)
    #         X_scaled, y_class = smote.fit_resample(X_scaled, y_class)

    #         # Rebalance y_reg (regression targets) with simple oversampling
    #         y_reg_resampled = []
    #         for label in sorted(y_class.unique()):
    #             label_indices = np.where(y_class == label)[0]
    #             if len(label_indices) > len(y_reg):
    #                 sample_rows = y_reg.sample(len(label_indices) - len(y_reg), replace=True, random_state=42)
    #                 y_reg_resampled.append(sample_rows)
    #         if y_reg_resampled:
    #             y_reg = pd.concat([y_reg] + y_reg_resampled, axis=0).reset_index(drop=True)
    
    
    
    # ÙŠØªØ­Ù‚Ù‚ Ù…Ù† Ø¹Ø¯Ø¯ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø¨ÙƒÙ„ ÙØ¦Ø©.
    # ÙŠØ³ØªØ®Ø¯Ù… SMOTE Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø¹ÙŠÙ†Ø§Øª ØµÙ†Ø§Ø¹ÙŠØ© ÙÙŠ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ø£Ù‚Ù„.
    # ÙŠØ¹ÙŠØ¯ ØªÙˆÙ„ÙŠØ¯ y_reg Ù„ØªØªÙˆØ§ÙÙ‚ Ù…Ø¹ ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©.
    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ ÙØ¦Ø§Øª Ø£Ù‚Ù„ Ù…Ù† 2ØŒ Ù„Ø§ ÙŠØ³ØªØ®Ø¯Ù… SMOTE.
    if use_smote:
        class_counts = y_class.value_counts()
        if all(class_counts >= 2):
            k_neighbors = max(1, min(class_counts.min() - 1, 5))
            smote = SMOTE(random_state=42, sampling_strategy='not majority', k_neighbors=k_neighbors)
            X_scaled, y_class_resampled = smote.fit_resample(X_scaled, y_class)

            # Ø§Ù„Ø¢Ù† Ù†Ø¹ÙŠØ¯ ØªÙˆÙ„ÙŠØ¯ y_reg Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø§Ø³Ø¨ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ y_class_resampled
            y_reg_resampled = []

            label_mapping = y_class.reset_index(drop=True)  # Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ø£ØµÙ„ÙŠ
            for label in sorted(y_class_resampled.unique()):
                n_samples = sum(y_class_resampled == label)
                available = y_reg[y_class == label]
                if available.empty:
                    continue
                sampled = available.sample(n_samples, replace=True, random_state=42)
                y_reg_resampled.append(sampled)

            y_reg = pd.concat(y_reg_resampled, axis=0).reset_index(drop=True)
            y_class = y_class_resampled

    # Save processed dataframe for analysis
    _processed_df = df.copy()

    # Split dataset
    # ØªÙ‚Ø³ÙŠÙ… Ø¥Ù„Ù‰:
    # ØªØ¯Ø±ÙŠØ¨
    # Ø§Ø®ØªØ¨Ø§Ø±
    # ÙƒÙ„ Ù…Ù† Ø§Ù„ØªØµÙ†ÙŠÙ ÙˆØ§Ù„ØªÙ†Ø¨Ø¤
    # stratify=y_class ÙŠØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ù†ÙØ³ ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙØ¦Ø§Øª ÙÙŠ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ØªÙŠÙ†.
    # test_size=0.3 ÙŠØ¹Ù†ÙŠ Ø£Ù† 30% Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø³ØªØ³ØªØ®Ø¯Ù… Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±.
    # random_state=42 Ù„Ø¶Ù…Ø§Ù† ØªÙƒØ±Ø§Ø± Ø§Ù„Ù†ØªØ§Ø¦Ø¬.
    X_train, X_test, y_train_class, y_test_class, y_train_reg, y_test_reg = train_test_split(
        X_scaled, y_class, y_reg, test_size=0.2, random_state=42, stratify=y_class
    )

    return X_train, X_test, y_train_class, y_test_class, y_train_reg, y_test_reg

# ØªØ­ÙØ¸ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ€ CSV.
def export_processed_data():
    """Save the processed dataframe as CSV"""
    global _processed_df
    if _processed_df is not None:
        os.makedirs("data", exist_ok=True)
        _processed_df.to_csv("earthquake_risk_predictor/data/processed_earthquakes.csv", index=False)
        print("âœ… Processed data saved to 'earthquake_risk_predictor/data/processed_earthquakes.csv'")


# ØªØ±Ø³Ù… ÙˆØªØ­ÙØ¸ ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙØ¦Ø§Øª (Bar chart).
def plot_severity_distribution():
    """Plot and save class distribution"""
    global _processed_df
    if _processed_df is not None:
        plt.figure(figsize=(8, 5))
        _processed_df['severity_label'].value_counts().sort_index().plot(kind='bar', color='skyblue', edgecolor='black')
        plt.title("Tehlike SÄ±nÄ±fÄ± DaÄŸÄ±lÄ±mÄ±")
        plt.xlabel("Tehlike Seviyesi")
        plt.ylabel("Adet")
        plt.grid(True, axis='y')
        plt.tight_layout()
        os.makedirs("data", exist_ok=True)
        plt.savefig("earthquake_risk_predictor/data/severity_distribution.png")
        print("ğŸ“Š Saved severity class distribution to 'earthquake_risk_predictor/data/severity_distribution.png'")